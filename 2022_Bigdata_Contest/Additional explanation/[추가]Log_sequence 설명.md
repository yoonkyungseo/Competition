### Log 데이터 정리

Log 데이터의 경우 고객이 신청서를 만들 때의 고객이 App을 사용한 일련의 기록을 나타내는 정보라고 할 수 있다.

따라서 해당 Log 데이터를 이용하고자 다음과 같은 PIPE LINE을 구성하였다.

1. 유저 아이디 별 조회를 위해 만든 신청서 List 확보
2. Log  데이터에 매핑되어 있는 대로 유저 별 로그 시퀀스 확보
   1. 해당 과정에서 로그 시퀀스를 확보하기 위해 Log 의 time stamp를 활용
      1. 한 user에서 나오는 Log 일련의 기록을 묶기 위해 각 로그 들의 Time stamp를 조회하여 Time Stamp 간의 시간 격차가 300초 이내라면 동일한 로그 시퀀스로 취급
      2. 위 과정을 반복하여 각 user 별 로그 시퀀스를 생성
      3. 본 데이터는 로그 시퀀스와 신청서가 매핑되어있지 않기 때문에 각 유저의 각 로그 시퀀스가 끝난 시점에서 격차가 300초 이내에서 생성된 신청서 번호의 경우, 해당 로그 시퀀스에서 나온 신청서 번호로 간주
      4. 최종적으로 각 유저에 대해 {로그 시퀀스: 신청서번호} 형태의 Dictionary 데이터 생성
   2. 로그 시퀀스 확보 결과 로그 데이터가 없는 신청서 번호가 user_sepc.csv 안에 존재하나, 전체 log_data.csv 에 들어가있는 데이터는 모두 로그 시퀀스로 묶임
   3. 해당 과정을 get_log1.py , get_log2.py 파일로 만들어 자동화
3. 이후 해당 Log 시퀀스가 포함하고 있는 정보를 판단하기 위해 RNN 계열의 모델을 통해 각 로그 시퀀스 데이터와 매핑된 신청서 번호가 실제로 loan 데이터 즉, 대출 조회까지 간 신청서인지 판단
   1. 로그 시퀀스의 데이터 중 지나치게 시퀀스가 긴 데이터가 있기 때문에 시퀀스 길이를 맞춰주기 위해 전체 시퀀스의 길이의 box plot을 그린 결과 약 98%의 시퀀스의 길이가 20 이하라는 결과를 얻어 최대 시퀀스의 길이를 20으로 제함
   2. 해당 과정에서 각 로그 명칭 별로 토큰화를 진행한 후 길이를 맞춰주기 위해 각 시퀀스 데이터에 대해 패딩 진행
   3. 총 user_spec 1394216개의 신청서 번호 데이터 중 로그 시퀀스 데이터가 없는 신청서 번호 데이터를 제외한 982189 개의 학습 데이터를 생성
   4. 해당 신청서 데이터에 대하여 실제 대출 조회까지 간 데이터인 672324 개의 데이터를 Positive 로 두어 이진 분류 모델 개발
   5. 모델은 다양한 실험을 하기 위해 Transformer기반 모델이 아닌 GRU 모델을 사용
      1. n_layers = 2, hidden_dims = 128, embedding_dims = 64, drop_out = 0.2 적용
      2. Adam optimizer를 사용하였으며 0.001의 learing rate로 총 100 Epoch 학습 진행
   6. 모델링 결과 92.89% 의 정확도를 얻음
4. 이후 해당 모델에서 각 시퀀스 별 정보를 뽑아내기 위해 GRU 를 거친 Vector를 $10\times 2$ 로 Projection 해주는 Linear Layer를 추가로 쌓음
5. 이후 학습된 모델을 바탕으로 해당 Linear Layer를 거치게 하여 각 시퀀스 당 10차원의 Vector를 추출함
6. 해당 Vector를 각 로그 시퀀스의 임베딩 벡터로 간주하여 차후 대출 신청 여부 예측과 Clustering 의 피처로 사용