{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown system...\n"
     ]
    }
   ],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# EDA\n",
    "#import klib\n",
    "\n",
    "# Preprocessing & Feature Engineering\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "#matplotlib 한글깨짐 지원\n",
    "import platform\n",
    "\n",
    "path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)\n",
    "else:\n",
    "    print('Unknown system...')\n",
    "rc('axes', unicode_minus=False)    \n",
    "\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "from sklearn.metrics import *\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from pytorch_tabnet.tab_model import TabNetClassifier \n",
    "# from pytorch_tabnet.metrics import Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data_train.csv')\n",
    "data_test = pd.read_csv('data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10270011, 49), (3257159, 49))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10270001, 49)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Down Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대용량의 데이터의 down-sampling을 위해 별도의 python 파일을 생성하여 터미널에서 작동.\n",
    "# 파일은 별도 첨부하겠음.\n",
    "# 코드는 아래와 같음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Undersampling.py`__\n",
    "```python\n",
    "from tqdm import tqdm\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "if __name__ == \"__main__\":\n",
    "    data_train = pd.read_csv('tr_feature_woo.csv',encoding='UTF-8')\n",
    "    data_test = pd.read_csv('te_feature_woo.csv',encoding='UTF-8')##\n",
    "    dat = data_train['신청서번호'].unique()\n",
    "    a = data_train.query('신청서번호 in @dat').groupby('신청서번호')['신청서번호'].count()\n",
    "    a = a.rename('신청수')\n",
    "    a = a.reset_index()\n",
    "    mini = data_train[['신청서번호','신청여부']]\n",
    "    rand_idx = []\n",
    "    index_rand = []\n",
    "    z=0\n",
    "    for j in tqdm(dat):\n",
    "        rnd_num = []\n",
    "        df = mini.query('신청서번호 == @j & 신청여부 == 0 ').index\n",
    "        num = a.query('신청서번호 == @j')['신청수']\n",
    "        if len(df) != 0:\n",
    "            if len(df) > num.values[0]:\n",
    "                if int(num.values[0]*0.85) > 5:\n",
    "                    rnd_num = list(np.random.choice(len(df), size  =int(num.values[0]*0.85),replace = False))\n",
    "                elif int(num.values[0]*0.85) > 3:\n",
    "                    rnd_num = list(np.random.choice(len(df), size  =int(num.values[0]/2),replace = False))\n",
    "                else:\n",
    "                    rnd_num = list(np.random.choice(len(df), size  = 1 ,replace = False))\n",
    "            else:\n",
    "                rnd_num = [i for i in range(len(df))]\n",
    "        else:\n",
    "            rnd_num = [-1234]\n",
    "        for idx in rnd_num:\n",
    "            if idx == -1234:\n",
    "                pass\n",
    "            else:\n",
    "                index_rand.append(df[idx])\n",
    "    with open(\"index_final.npy\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(index_rand, fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling.py 에서 나온 index_final.npy 의 index 제거\n",
    "idx = np.load('index_final.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop(index = idx).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6061365, 49)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6061374, 49), (3257159, 49))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6061374, 49)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120672, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train[['유저번호','신청서번호','금융사번호', '상품번호', '승인한도', '승인금리', '유저성별', '한도조회당시유저신용점수', '연소득', '근로형태','입사연월', '고용형태', '주거소유형태', '대출희망금액', '대출목적', '개인회생자여부', '개인회생자납입완료여부','기대출수', '기대출금액', '한도조회월', '상품코드', '유저타입', '상품매력도', '연이자부담지수','3-5월평균순위변동폭', '6월예상순위', '연소득증감률', '과대출자', '기출등급', '나이', '앱버전','앱버전_세부내역', '신청서조회_비율', '신용점수_증감률']]\n",
    "X_test = data_test[['유저번호','신청서번호','금융사번호', '상품번호', '승인한도', '승인금리', '유저성별', '한도조회당시유저신용점수', '연소득', '근로형태','입사연월', '고용형태', '주거소유형태', '대출희망금액', '대출목적', '개인회생자여부', '개인회생자납입완료여부','기대출수', '기대출금액', '한도조회월', '상품코드', '유저타입', '상품매력도', '연이자부담지수','3-5월평균순위변동폭', '6월예상순위', '연소득증감률', '과대출자', '기출등급', '나이', '앱버전','앱버전_세부내역', '신청서조회_비율', '신용점수_증감률']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train[['금융사번호', '상품번호', '승인한도', '승인금리', '유저성별', '한도조회당시유저신용점수', '연소득', '근로형태','입사연월', '고용형태', '주거소유형태', '대출희망금액', '대출목적', '개인회생자여부', '개인회생자납입완료여부','기대출수', '기대출금액', '한도조회월', '상품코드', '유저타입', '상품매력도', '연이자부담지수','3-5월평균순위변동폭', '6월예상순위', '연소득증감률', '과대출자', '기출등급', '나이', '앱버전','앱버전_세부내역', '신청서조회_비율', '신용점수_증감률']]\n",
    "X_test = data_test[['금융사번호', '상품번호', '승인한도', '승인금리', '유저성별', '한도조회당시유저신용점수', '연소득', '근로형태','입사연월', '고용형태', '주거소유형태', '대출희망금액', '대출목적', '개인회생자여부', '개인회생자납입완료여부','기대출수', '기대출금액', '한도조회월', '상품코드', '유저타입', '상품매력도', '연이자부담지수','3-5월평균순위변동폭', '6월예상순위', '연소득증감률', '과대출자', '기출등급', '나이', '앱버전','앱버전_세부내역', '신청서조회_비율', '신용점수_증감률']]\n",
    "y_train = data_train['신청여부']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['금융사번호','상품번호','유저성별','근로형태','입사연월','고용형태','주거소유형태','대출목적','개인회생자여부','개인회생자납입완료여부','한도조회월','상품코드','유저타입','과대출자','기출등급','나이','앱버전','앱버전_세부내역','']\n",
    "num_features = list(set(X_train.columns) - set(cat_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    560336\n",
       "1.0    560336\n",
       "Name: 신청여부, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y class들의 불균형을 잡아주기 위한 random_sampling\n",
    "from sklearn.utils import resample\n",
    "X=pd.concat([X_train,y_train],axis=1)\n",
    "\n",
    "\n",
    "not_app=X[X.신청여부==0]\n",
    "app=X[X.신청여부==1]\n",
    "\n",
    "\n",
    "\n",
    "not_fraud_downsampled = resample(not_app,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = int(len(app)), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, app])\n",
    "\n",
    "# checking counts\n",
    "downsampled.신청여부.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = downsampled[X_train.columns.tolist()]\n",
    "y_train = downsampled['신청여부']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1120672, 34), (1120672,), (3257159, 34))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_holdout = False\n",
    "n_splits = 5\n",
    "iterations = 1000\n",
    "patience = 50\n",
    "\n",
    "cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "scv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    " \n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Learning rate set to 0.039954\n",
      "0:\tlearn: 0.7854368\ttest: 0.7884320\tbest: 0.7884320 (0)\ttotal: 44.6ms\tremaining: 44.6s\n",
      "100:\tlearn: 0.8376331\ttest: 0.8407072\tbest: 0.8407072 (100)\ttotal: 3.88s\tremaining: 34.6s\n",
      "200:\tlearn: 0.8486094\ttest: 0.8534652\tbest: 0.8534652 (200)\ttotal: 7.62s\tremaining: 30.3s\n",
      "300:\tlearn: 0.8533182\ttest: 0.8586699\tbest: 0.8586936 (299)\ttotal: 11.3s\tremaining: 26.2s\n",
      "400:\tlearn: 0.8564484\ttest: 0.8616904\tbest: 0.8617240 (399)\ttotal: 14.9s\tremaining: 22.3s\n",
      "500:\tlearn: 0.8588787\ttest: 0.8643679\tbest: 0.8643679 (500)\ttotal: 18.5s\tremaining: 18.5s\n",
      "600:\tlearn: 0.8609467\ttest: 0.8664596\tbest: 0.8665249 (598)\ttotal: 22.2s\tremaining: 14.7s\n",
      "700:\tlearn: 0.8625730\ttest: 0.8683107\tbest: 0.8683107 (700)\ttotal: 25.9s\tremaining: 11s\n",
      "800:\tlearn: 0.8640020\ttest: 0.8698139\tbest: 0.8698290 (797)\ttotal: 29.6s\tremaining: 7.35s\n",
      "900:\tlearn: 0.8651148\ttest: 0.8708542\tbest: 0.8708542 (900)\ttotal: 33.3s\tremaining: 3.66s\n",
      "999:\tlearn: 0.8659978\ttest: 0.8721012\tbest: 0.8721197 (997)\ttotal: 36.9s\tremaining: 0us\n",
      "bestTest = 0.8721197355\n",
      "bestIteration = 997\n",
      "Shrink model to first 998 iterations.\n",
      "==================================================\n",
      "Learning rate set to 0.039954\n",
      "0:\tlearn: 0.7842435\ttest: 0.7877570\tbest: 0.7877570 (0)\ttotal: 44.4ms\tremaining: 44.4s\n",
      "100:\tlearn: 0.8371801\ttest: 0.8400331\tbest: 0.8400331 (100)\ttotal: 3.9s\tremaining: 34.7s\n",
      "200:\tlearn: 0.8483633\ttest: 0.8520093\tbest: 0.8520093 (200)\ttotal: 7.6s\tremaining: 30.2s\n",
      "300:\tlearn: 0.8539695\ttest: 0.8580378\tbest: 0.8580378 (300)\ttotal: 11.3s\tremaining: 26.2s\n",
      "400:\tlearn: 0.8569387\ttest: 0.8616712\tbest: 0.8616712 (400)\ttotal: 15s\tremaining: 22.4s\n",
      "500:\tlearn: 0.8593971\ttest: 0.8643926\tbest: 0.8643926 (500)\ttotal: 18.7s\tremaining: 18.6s\n",
      "600:\tlearn: 0.8612282\ttest: 0.8662629\tbest: 0.8662629 (600)\ttotal: 22.4s\tremaining: 14.9s\n",
      "700:\tlearn: 0.8626806\ttest: 0.8679061\tbest: 0.8679602 (692)\ttotal: 26.1s\tremaining: 11.1s\n",
      "800:\tlearn: 0.8639221\ttest: 0.8695561\tbest: 0.8695940 (797)\ttotal: 29.7s\tremaining: 7.39s\n",
      "900:\tlearn: 0.8650835\ttest: 0.8709465\tbest: 0.8709465 (900)\ttotal: 33.5s\tremaining: 3.68s\n",
      "999:\tlearn: 0.8661572\ttest: 0.8718394\tbest: 0.8718394 (999)\ttotal: 37.1s\tremaining: 0us\n",
      "bestTest = 0.8718393762\n",
      "bestIteration = 999\n",
      "==================================================\n",
      "Learning rate set to 0.039954\n",
      "0:\tlearn: 0.7875200\ttest: 0.7888996\tbest: 0.7888996 (0)\ttotal: 44.3ms\tremaining: 44.3s\n",
      "100:\tlearn: 0.8386450\ttest: 0.8410196\tbest: 0.8410196 (100)\ttotal: 3.88s\tremaining: 34.5s\n",
      "200:\tlearn: 0.8493717\ttest: 0.8525135\tbest: 0.8525135 (200)\ttotal: 7.58s\tremaining: 30.1s\n",
      "300:\tlearn: 0.8542786\ttest: 0.8583620\tbest: 0.8584206 (299)\ttotal: 11.2s\tremaining: 26.1s\n",
      "400:\tlearn: 0.8571295\ttest: 0.8614220\tbest: 0.8614257 (398)\ttotal: 14.9s\tremaining: 22.2s\n",
      "500:\tlearn: 0.8595651\ttest: 0.8636647\tbest: 0.8636647 (500)\ttotal: 18.6s\tremaining: 18.5s\n",
      "600:\tlearn: 0.8614754\ttest: 0.8655175\tbest: 0.8655584 (599)\ttotal: 22.3s\tremaining: 14.8s\n",
      "700:\tlearn: 0.8628855\ttest: 0.8670862\tbest: 0.8670862 (700)\ttotal: 25.9s\tremaining: 11s\n",
      "800:\tlearn: 0.8641661\ttest: 0.8686390\tbest: 0.8686390 (800)\ttotal: 29.6s\tremaining: 7.35s\n",
      "900:\tlearn: 0.8653127\ttest: 0.8697701\tbest: 0.8697701 (900)\ttotal: 33.2s\tremaining: 3.65s\n",
      "999:\tlearn: 0.8662964\ttest: 0.8708349\tbest: 0.8708443 (998)\ttotal: 36.9s\tremaining: 0us\n",
      "bestTest = 0.8708442967\n",
      "bestIteration = 998\n",
      "Shrink model to first 999 iterations.\n",
      "==================================================\n",
      "Learning rate set to 0.039954\n",
      "0:\tlearn: 0.7861310\ttest: 0.7880526\tbest: 0.7880526 (0)\ttotal: 44.4ms\tremaining: 44.4s\n",
      "100:\tlearn: 0.8379039\ttest: 0.8383532\tbest: 0.8383532 (100)\ttotal: 3.84s\tremaining: 34.2s\n",
      "200:\tlearn: 0.8493680\ttest: 0.8515400\tbest: 0.8515400 (200)\ttotal: 7.58s\tremaining: 30.1s\n",
      "300:\tlearn: 0.8542347\ttest: 0.8572540\tbest: 0.8572614 (299)\ttotal: 11.2s\tremaining: 26.1s\n",
      "400:\tlearn: 0.8570386\ttest: 0.8604970\tbest: 0.8604970 (400)\ttotal: 14.9s\tremaining: 22.3s\n",
      "500:\tlearn: 0.8593687\ttest: 0.8627867\tbest: 0.8627867 (500)\ttotal: 18.6s\tremaining: 18.5s\n",
      "600:\tlearn: 0.8612173\ttest: 0.8647790\tbest: 0.8647790 (600)\ttotal: 22.3s\tremaining: 14.8s\n",
      "700:\tlearn: 0.8627893\ttest: 0.8664541\tbest: 0.8664767 (699)\ttotal: 25.9s\tremaining: 11.1s\n",
      "800:\tlearn: 0.8640231\ttest: 0.8675695\tbest: 0.8676177 (795)\ttotal: 29.6s\tremaining: 7.36s\n",
      "900:\tlearn: 0.8651811\ttest: 0.8688398\tbest: 0.8688398 (900)\ttotal: 33.3s\tremaining: 3.66s\n",
      "999:\tlearn: 0.8662325\ttest: 0.8700858\tbest: 0.8700858 (999)\ttotal: 37s\tremaining: 0us\n",
      "bestTest = 0.8700857608\n",
      "bestIteration = 999\n",
      "==================================================\n",
      "Learning rate set to 0.039954\n",
      "0:\tlearn: 0.7855938\ttest: 0.7864772\tbest: 0.7864772 (0)\ttotal: 44.7ms\tremaining: 44.7s\n",
      "100:\tlearn: 0.8377130\ttest: 0.8395223\tbest: 0.8395223 (100)\ttotal: 3.85s\tremaining: 34.2s\n",
      "200:\tlearn: 0.8487448\ttest: 0.8514621\tbest: 0.8514621 (200)\ttotal: 7.61s\tremaining: 30.3s\n",
      "300:\tlearn: 0.8537269\ttest: 0.8567284\tbest: 0.8567284 (300)\ttotal: 11.3s\tremaining: 26.3s\n",
      "400:\tlearn: 0.8569405\ttest: 0.8605102\tbest: 0.8605102 (400)\ttotal: 15s\tremaining: 22.4s\n",
      "500:\tlearn: 0.8591868\ttest: 0.8630245\tbest: 0.8630507 (498)\ttotal: 18.7s\tremaining: 18.6s\n",
      "600:\tlearn: 0.8612682\ttest: 0.8650803\tbest: 0.8650881 (599)\ttotal: 22.4s\tremaining: 14.9s\n",
      "700:\tlearn: 0.8629827\ttest: 0.8672395\tbest: 0.8672395 (700)\ttotal: 26.2s\tremaining: 11.2s\n",
      "800:\tlearn: 0.8642759\ttest: 0.8686003\tbest: 0.8686223 (799)\ttotal: 29.9s\tremaining: 7.43s\n",
      "900:\tlearn: 0.8655000\ttest: 0.8698482\tbest: 0.8698532 (899)\ttotal: 33.6s\tremaining: 3.69s\n",
      "999:\tlearn: 0.8664192\ttest: 0.8705526\tbest: 0.8706311 (994)\ttotal: 37.3s\tremaining: 0us\n",
      "bestTest = 0.87063114\n",
      "bestIteration = 994\n",
      "Shrink model to first 995 iterations.\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "models = []\n",
    "\n",
    "\n",
    "models = []\n",
    "#for tri, vai in cv.split(X_train):          #KFold\n",
    "for tri, vai in scv.split(X_train,y_train): #StratifiedKFold\n",
    "    print(\"=\"*50)\n",
    "    preds = []\n",
    "\n",
    "    model = CatBoostClassifier(iterations=iterations,\n",
    "                               random_state=42,\n",
    "                               task_type=\"GPU\",\n",
    "                               eval_metric=\"F1\",\n",
    "                               cat_features=cat_features,\n",
    "                               one_hot_max_size=4,\n",
    "                               #learning_rate=0.05\n",
    "                              )\n",
    "    model.fit(X_train.iloc[tri], y_train.iloc[tri], \n",
    "            eval_set=[(X_train.iloc[vai], y_train.iloc[vai])], \n",
    "            early_stopping_rounds=patience ,\n",
    "            use_best_model=True,\n",
    "            verbose = 100\n",
    "        )\n",
    "    \n",
    "    models.append(model)\n",
    "    scores.append(model.get_best_score()[\"validation\"][\"F1\"])\n",
    "    if is_holdout:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score에 대한 정리\n",
    "#average의 default는 binary, binary인 경우 pos_label이라는 파라미터 값이 default로 1이 되는데 이경우 라벨값이 1인경우의 f1 score만을 산출함\n",
    "#즉, 1과 0의 total f1 score가 아닌 1의 f1 score가 출력됨\n",
    "#average=\"micro\" : 일반적으로 우리가 아는 총합 정밀도와 재현율을 통해 구하는 f1 score\n",
    "#average=\"macro\" : label별로 가중을 두지 않고 각각 정밀도, 재현율, f1 score를 구해서 평균을 낸 정밀도, 재현율을 통해 f1 score를 출력함\n",
    "#average=\"weighted\" : macro인 경우에서 소수 label에 가중을 두고 구한 f1 score\n",
    "\n",
    "#Catboost의 F1 score는 average = \"micro\" 인것으로 판단됨(일반적으로 우리가 아는 총합 f1 score 산출방식)\n",
    "\n",
    "def score_function(real, pred):\n",
    "    score = f1_score(real, pred, average=\"micro\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8688647466928414, 0.8688825930800633, 0.8679450685750488, 0.867057206849474, 0.8676862948057859]\n",
      "0.8680871820006427\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "scores = []\n",
    "\n",
    "#for i,(tri, vai) in enumerate(cv.split(X_train)):                #KFold\n",
    "for i,(tri, vai) in enumerate(scv.split(X_train,y_train)):       #StratifiedKFold\n",
    "#    pred = models[i].predict_proba(X_train.iloc[vai])[:, 1]\n",
    "    pred = models[i].predict(X_train.iloc[vai])\n",
    "#    pred = np.where(pred >= threshold , 1, 0)\n",
    "    score = score_function(y_train.iloc[vai],pred)\n",
    "    scores.append(score)\n",
    "#    pred = models[i].predict_proba(X_test)[:, 1]\n",
    "    preds.append(pred)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.where(pred >= threshold , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.89      0.84      0.86    112067\n",
      "     class 1       0.85      0.89      0.87    112067\n",
      "\n",
      "    accuracy                           0.87    224134\n",
      "   macro avg       0.87      0.87      0.87    224134\n",
      "weighted avg       0.87      0.87      0.87    224134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train.iloc[vai], pred, target_names=['class 0', 'class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(n_estimators = 1000, random_state=0)\n",
    "\n",
    "lgbm_model = lgbm.fit(X_train,y_train)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "score_lgbm = cross_val_score(lgbm_model,X_train,y_train,scoring='f1',cv=skf,n_jobs=-1)\n",
    "\n",
    "print(score_lgbm)\n",
    "print(f'최대성능: {max(score_lgbm)}\\n평균성능: {np.mean(score_lgbm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model.fit(X_train, y_train)\n",
    "joblib.dump(lgbm_model,'lgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf_model = rf.fit(X_train,y_train)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "score_rf = cross_val_score(rf_model,X_train,y_train,scoring='f1',cv=skf,n_jobs=-1)\n",
    "\n",
    "print(score_rf)\n",
    "print(f'최대성능: {max(score_rf)}\\n평균성능: {np.mean(score_rf)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train, y_train)\n",
    "joblib.dump(rf_model,'rf_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X_train.copy()\n",
    "X_te = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_tr[num_features] = scaler.fit_transform(X_tr[num_features])\n",
    "X_te[num_features] = scaler.transform(X_te[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_dev, y_train_, y_dev = train_test_split(X_tr, y_train, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매번 모델링을 할 때마다 동일한 결과를 얻기 위해 랜덤 시드 설정 동일하게 유지\n",
    "# 럭키 시드 찾는건 시간상 어려움\n",
    "\n",
    "# tensorflow 패키지\n",
    "#!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import re\n",
    "\n",
    "def reset_seeds(reset_graph_with_backend=None):\n",
    "    if reset_graph_with_backend is not None:\n",
    "        K = reset_graph_with_backend\n",
    "        K.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        print(\"KERAS AND TENSORFLOW GRAPHS RESET\")  # optional\n",
    "\n",
    "    np.random.seed(1)\n",
    "    random.seed(2)\n",
    "    tf.compat.v1.set_random_seed(3)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # for GPU\n",
    "    print(\"RANDOM SEEDS RESET\")  # optional\n",
    "   \n",
    "reset_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "input = keras.Input(shape=(X_train_.shape[1],))\n",
    "\n",
    "x = keras.layers.Dense(256, activation='relu')(input) # 은닉층 낮추기\n",
    "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "DEEP = keras.Model(input, output)\n",
    "\n",
    "DEEP.summary()\n",
    "\n",
    "#Image(keras.utils.model_to_dot(DEEP,show_shapes=True,show_layer_names=False, dpi=100).create(prog='dot', format='png'))\n",
    "\n",
    "DEEP.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', keras.metrics.AUC()])\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience = 5),] # patience 낮추기\n",
    "\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "hist = DEEP.fit(X_train_, y_train_, validation_data=(X_dev, y_dev), \n",
    "                 batch_size=1024, epochs=50, callbacks=[callbacks, mc], shuffle=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# compile the model\n",
    "DEEP.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "# fit the model\n",
    "history = DEEP.fit(X_train_, y_train_, validation_split=0.3, epochs=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = DEEP.evaluate(X_dev, y_dev, verbose=0)\n",
    "\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEP.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(DEEP,'dnn_model.pkl') \n",
    "DEEP.save('dnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120672, 32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.drop(columns = ['신청서번호','한도조회일시','유저번호','유저생년월일','생성일시','신청여부','한도조회월일','0','1','2','3','4','5','6','7','8','9'])\n",
    "y_train = data_train['신청여부']\n",
    "X_test = data_test.drop(columns = ['신청서번호','한도조회일시','유저번호','유저생년월일','생성일시','신청여부','한도조회월일','0','1','2','3','4','5','6','7','8','9'])\n",
    "tr_application_id = data_train['신청서번호']\n",
    "te_application_id = data_test['신청서번호']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6061374, 32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    560336\n",
       "1.0    560336\n",
       "Name: 신청여부, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y class들의 불균형을 잡아주기 위한 random_sampling\n",
    "from sklearn.utils import resample\n",
    "X=pd.concat([X_train,y_train],axis=1)\n",
    "\n",
    "\n",
    "not_app=X[X.신청여부==0]\n",
    "app=X[X.신청여부==1]\n",
    "\n",
    "\n",
    "\n",
    "not_fraud_downsampled = resample(not_app,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = int(len(app)), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, app])\n",
    "\n",
    "# checking counts\n",
    "downsampled.신청여부.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = downsampled[X_train.columns.tolist()]\n",
    "y_train = downsampled['신청여부']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "cat_model = joblib.load('models/cat_model.pkl')\n",
    "lgbm_model = joblib.load('models/lgbm_model.pkl')\n",
    "rf_model = joblib.load('models/rf_model.pkl')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "dnn_model = tf.keras.models.load_model('best_model.h5')\n",
    "\n",
    "cat_features =  ['금융사번호','상품번호','유저성별','근로형태','입사연월','고용형태','주거소유형태','대출목적','개인회생자여부','개인회생자납입완료여부','한도조회월','상품코드','유저타입','과대출자','기출등급','나이','mp_app_version','detail']\n",
    "num_features = list(set(X_train.columns) - set(cat_features))\n",
    "\n",
    "\n",
    "preds = []\n",
    "pred_cat = cat_model.predict_proba(X_test)[:,1]\n",
    "preds.append(pred_cat)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for i in cat_features:\n",
    "    X_train[i] = LabelEncoder().fit_transform(X_train[i])\n",
    "\n",
    "for i in cat_features:\n",
    "    X_test[i] = LabelEncoder().fit_transform(X_test[i])\n",
    "\n",
    "pred_lgb = lgbm_model.predict_proba(X_test)[:,1]\n",
    "preds.append(pred_lgb)\n",
    "\n",
    "#pred_rf = rf_model.predict_proba(X_test)[:.1]\n",
    "#preds.append(pred_rf)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_tr = X_train.copy()\n",
    "X_te = X_test.copy()\n",
    "scaler = StandardScaler()\n",
    "X_tr[num_features] = scaler.fit_transform(X_tr[num_features])\n",
    "X_te[num_features] = scaler.transform(X_te[num_features])\n",
    "\n",
    "pred_dnn = dnn_model.predict(X_te).flatten()\n",
    "preds.appned(pred_dnn)\n",
    "\n",
    "threshold = 0.5\n",
    "preds = np.mean(preds , axis = 0 )\n",
    "pred = np.where(preds > threshold,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터에 신청여부 predict 값 concat\n",
    "a = pd.DataFrame({'신청여부' : pred})\n",
    "data_test = data_test.drop(columns = '신청여부')\n",
    "data_test = pd.concat([data_test, a], axis = 1)\n",
    "\n",
    "submission = pd.read_csv('데이터분석분야_퓨처스부문_평가데이터.csv')\n",
    "\n",
    "submission_test = data_test[['신청서번호','상품번호','신청여부']]\n",
    "submission_test.rename(columns={'신청서번호':'application_id','상품번호':'product_id','신청여부':'is_applied_new'},inplace=True)\n",
    "\n",
    "submission = pd.merge(submission, submission_test, how='left', on=['application_id','product_id'])\n",
    "\n",
    "submission.drop(columns=['is_applied'],inplace=True)\n",
    "submission.rename(columns={'is_applied_new':'is_applied'},inplace=True)\n",
    "\n",
    "submission.to_csv('평가데이터정답지.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Catboost\n",
    "model = joblib.load('cat_model.pkl')\n",
    "\n",
    "predictors = ['금융사번호', '상품번호', '승인한도', '승인금리', '유저성별', '한도조회당시유저신용점수', '연소득', '근로형태',\n",
    "       '입사연월', '고용형태', '주거소유형태', '대출희망금액', '대출목적', '개인회생자여부', '개인회생자납입완료여부',\n",
    "       '기대출수', '기대출금액', '한도조회월', '상품코드', '유저타입', '상품매력도', '연이자부담지수',\n",
    "       '3-5월평균순위변동폭', '6월예상순위', '연소득증감률', '과대출자', '기출등급', '나이', '앱버전',\n",
    "       '앱버전_세부내역', '신청서조회_비율', '신용점수_증감률']\n",
    "\n",
    "tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': model.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGBM\n",
    "import joblib\n",
    "\n",
    "model = joblib.load('lgbm_model.pkl')\n",
    "\n",
    "tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': model.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "model = joblib.load('rf_model.pkl')\n",
    "\n",
    "tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': model.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X[num_features])\n",
    "#X_test[num_features] = scaler.transform(X_test[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=10, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 3\n",
    "# 그룹 수, random_state 설정\n",
    "model = KMeans(n_clusters = k, random_state = 10)\n",
    "# 정규화된 데이터에 학습\n",
    "model.fit(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['cluster'] = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.concat([data_test['신청여부'], downsampled['신청여부']], axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X, target], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('data/for_EDA.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.9.0 on Python 3.8 (CUDA 11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
